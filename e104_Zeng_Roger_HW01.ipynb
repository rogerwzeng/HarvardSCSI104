{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rogerwzeng/HarvardSCSI104/blob/main/e104_Zeng_Roger_HW01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0H8fVx5iu6K"
      },
      "source": [
        "This code is from the book: \n",
        "\n",
        "**Hands-On Machine Learning with Scikit-Learn, Keras and tensorFlow 2, by Aurelien Geron, O'Reilly, 2019**\n",
        "\n",
        "Chapter 19 – Training and Deploying TensorFlow Models at Scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HeyaA0ciu6N"
      },
      "source": [
        "_This notebook contains parts of the sample code in chapter 19._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVIQADxaiu6P"
      },
      "source": [
        "# Setup\n",
        "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-26T08:31:56.193252Z",
          "start_time": "2023-01-26T08:31:52.592528Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1smylIuiu6Q",
        "outputId": "ae045719-4b53-48e0-b4fd-543c7c5b1c55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  2943  100  2943    0     0  33443      0 --:--:-- --:--:-- --:--:-- 33443\n",
            "OK\n",
            "Get:1 http://storage.googleapis.com/tensorflow-serving-apt stable InRelease [3,026 B]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Get:10 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server-universal amd64 Packages [349 B]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Get:12 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server amd64 Packages [340 B]\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Get:17 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [986 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2,009 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,439 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,290 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2,920 kB]\n",
            "Fetched 9,988 kB in 3s (2,894 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "39 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tensorflow-model-server\n",
            "0 upgraded, 1 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 414 MB of archives.\n",
            "After this operation, 0 B of additional disk space will be used.\n",
            "Get:1 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server amd64 tensorflow-model-server all 2.11.0 [414 MB]\n",
            "Fetched 414 MB in 11s (38.1 MB/s)\n",
            "Selecting previously unselected package tensorflow-model-server.\n",
            "(Reading database ... 129502 files and directories currently installed.)\n",
            "Preparing to unpack .../tensorflow-model-server_2.11.0_all.deb ...\n",
            "Unpacking tensorflow-model-server (2.11.0) ...\n",
            "Setting up tensorflow-model-server (2.11.0) ...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 KB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Is this notebook running on Colab or Kaggle?\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
        "\n",
        "if IS_COLAB or IS_KAGGLE:\n",
        "    !echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" > /etc/apt/sources.list.d/tensorflow-serving.list\n",
        "    !curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -\n",
        "    !apt update && apt-get install -y tensorflow-model-server\n",
        "    %pip install -q -U tensorflow-serving-api\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "    if IS_KAGGLE:\n",
        "        print(\"Go to Settings > Accelerator and select GPU.\")\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"deploy\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePeH6pnUiu6S"
      },
      "source": [
        "Once you have installed the GPU card(s) and all the required drivers and libraries,\n",
        "you can use the nvidia-smi command to check that CUDA is properly installed. It\n",
        "lists the available GPU cards, as well as processes running on each card:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-26T08:34:07.884737Z",
          "start_time": "2023-01-26T08:34:07.728676Z"
        },
        "id": "bKq_ehxuiu6T",
        "outputId": "789faf74-449a-4548-80a3-c0fa0ddadfa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jan 27 04:54:49 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    42W / 400W |      3MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrbapjOpiu6T"
      },
      "source": [
        "## Test for the presence of GPUs from Python code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eexM5zPeiu6T"
      },
      "source": [
        "**Note**: `tf.test.is_gpu_available()` is deprecated. Instead, use `tf.config.list_physical_devices('GPU')`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-26T08:34:14.294043Z",
          "start_time": "2023-01-26T08:34:14.284202Z"
        },
        "id": "qkpZk1VUiu6U",
        "outputId": "34241b92-a4c2-4465-979d-ce10844d0863",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#tf.test.is_gpu_available() # deprecated\n",
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-26T08:34:17.321638Z",
          "start_time": "2023-01-26T08:34:17.299168Z"
        },
        "id": "0nfHO2Jgiu6V",
        "outputId": "7b46ebe6-4185-41f9-9cd9-7b62fab1ed06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-26T08:34:20.126410Z",
          "start_time": "2023-01-26T08:34:20.118571Z"
        },
        "id": "9vReAr53iu6W",
        "outputId": "885fb00c-b021-41c6-9206-b3c8bbaa1228",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "tf.test.is_built_with_cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkK2Cw__iu6W"
      },
      "source": [
        "The `list_physical_devices()` function returns the list of all available\n",
        "GPU devices (just one in this example).11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-26T08:34:36.524705Z",
          "start_time": "2023-01-26T08:34:36.518828Z"
        },
        "id": "a2tHIviwiu6W",
        "outputId": "8bd74438-e055-41fc-80c1-238e549c3f20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 15939285289574986983\n",
              " xla_global_id: -1, name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 40215117824\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 13584960362285827471\n",
              " physical_device_desc: \"device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\"\n",
              " xla_global_id: 416903419]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from tensorflow.python.client.device_lib import list_local_devices\n",
        "\n",
        "devices = list_local_devices()\n",
        "devices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Kg6m4Vdiu6X"
      },
      "source": [
        "### Save/Load a SavedModel\n",
        "In the following exampple wre will use the standard MNIST model provided as `keras.dataset.mnist`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-26T08:35:07.401490Z",
          "start_time": "2023-01-26T08:35:01.089082Z"
        },
        "id": "xitv8jVxiu6X",
        "outputId": "d3c05473-ff65-43ef-bbe4-17cd6ce2f553",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "X_train_full = X_train_full[..., np.newaxis].astype(np.float32) / 255.\n",
        "X_test = X_test[..., np.newaxis].astype(np.float32) / 255.\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "X_new = X_test[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a2L7J8jiu6Y"
      },
      "source": [
        "## Distributed Training\n",
        "Many models can be trained quite well on a single GPU, or even on a CPU. But if\n",
        "training is too slow, you can try distributing it across multiple GPUs on the same\n",
        "machine. If that’s still too slow, try using more powerful GPUs, or add more GPUs to\n",
        "the machine. If your model performs heavy computations (such as large matrix multiplications),\n",
        "then it will run much faster on powerful GPUs, and you could even try\n",
        "to use TPUs on Google Cloud AI Platform, which will usually run even faster for such\n",
        "models. But if you can’t fit any more GPUs on the same machine, and if TPUs aren’t\n",
        "for you (e.g., perhaps your model doesn’t benefit much from TPUs, or perhaps you\n",
        "want to use your own hardware infrastructure), then you can try training it across\n",
        "several servers, each with multiple GPUs (if this is still not enough, as a last resort you\n",
        "can try adding some model parallelism, but this requires a lot more effort)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-26T08:35:23.364416Z",
          "start_time": "2023-01-26T08:35:23.332489Z"
        },
        "id": "A8ShsFNUiu6Z"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LieUfhGniu6a"
      },
      "source": [
        "We will first create and train a standard 2D convolutional model which could be run on either CPU or GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-26T08:35:49.899656Z",
          "start_time": "2023-01-26T08:35:49.881769Z"
        },
        "id": "ua2XhF1uiu6a"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    return keras.models.Sequential([\n",
        "        keras.layers.Conv2D(filters=64, kernel_size=7, activation=\"relu\",\n",
        "                            padding=\"same\", input_shape=[28, 28, 1]),\n",
        "        keras.layers.MaxPooling2D(pool_size=2),\n",
        "        keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\",\n",
        "                            padding=\"same\"), \n",
        "        keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\",\n",
        "                            padding=\"same\"),\n",
        "        keras.layers.MaxPooling2D(pool_size=2),\n",
        "        keras.layers.Flatten(),\n",
        "        keras.layers.Dense(units=64, activation='relu'),\n",
        "        keras.layers.Dropout(0.5),\n",
        "        keras.layers.Dense(units=10, activation='softmax'),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-26T09:05:47.675948Z",
          "start_time": "2023-01-26T08:35:53.856208Z"
        },
        "id": "B64coD23iu6a",
        "outputId": "9b2f2084-c466-4ca5-eb8b-451df5c44c60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "550/550 [==============================] - 12s 4ms/step - loss: 1.3434 - accuracy: 0.5520 - val_loss: 0.3747 - val_accuracy: 0.8978\n",
            "Epoch 2/10\n",
            "550/550 [==============================] - 2s 4ms/step - loss: 0.5005 - accuracy: 0.8451 - val_loss: 0.2199 - val_accuracy: 0.9382\n",
            "Epoch 3/10\n",
            "550/550 [==============================] - 2s 4ms/step - loss: 0.3394 - accuracy: 0.8987 - val_loss: 0.1539 - val_accuracy: 0.9526\n",
            "Epoch 4/10\n",
            "550/550 [==============================] - 2s 4ms/step - loss: 0.2645 - accuracy: 0.9221 - val_loss: 0.1137 - val_accuracy: 0.9684\n",
            "Epoch 5/10\n",
            "550/550 [==============================] - 2s 4ms/step - loss: 0.2148 - accuracy: 0.9370 - val_loss: 0.0962 - val_accuracy: 0.9726\n",
            "Epoch 6/10\n",
            "550/550 [==============================] - 2s 4ms/step - loss: 0.1861 - accuracy: 0.9459 - val_loss: 0.0783 - val_accuracy: 0.9756\n",
            "Epoch 7/10\n",
            "550/550 [==============================] - 2s 4ms/step - loss: 0.1647 - accuracy: 0.9517 - val_loss: 0.0722 - val_accuracy: 0.9778\n",
            "Epoch 8/10\n",
            "550/550 [==============================] - 2s 4ms/step - loss: 0.1490 - accuracy: 0.9573 - val_loss: 0.0677 - val_accuracy: 0.9788\n",
            "Epoch 9/10\n",
            "550/550 [==============================] - 2s 4ms/step - loss: 0.1351 - accuracy: 0.9605 - val_loss: 0.0635 - val_accuracy: 0.9814\n",
            "Epoch 10/10\n",
            "550/550 [==============================] - 2s 4ms/step - loss: 0.1253 - accuracy: 0.9636 - val_loss: 0.0599 - val_accuracy: 0.9822\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f362a3c56a0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "batch_size = 100\n",
        "model = create_model()\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(X_train, y_train, epochs=10,\n",
        "          validation_data=(X_valid, y_valid), batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yimDEGhYiu6b"
      },
      "source": [
        "TensorFlow comes with a very simple API that takes care of all the complexity\n",
        "for you: the Distribution Strategies API. To train a Keras model across all available\n",
        "GPUs (on a single machine, for now) using data parallelism with the mirrored\n",
        "strategy, create a `MirroredStrategy` object, call its `scope()` method to get a distribution\n",
        "context, and wrap the creation and compilation of your model inside that context.\n",
        "Then call the model’s `fit()` method normally.\n",
        "\n",
        "The following cell lets us choose one of severla options depending on the hardware configuration.\n",
        "\n",
        "Under the hood, tf.keras is distribution-aware, so in this MirroredStrategy context it\n",
        "knows that it must replicate all variables and operations across all available GPU\n",
        "devices. Note that the fit() method will automatically split each training batch\n",
        "across all the replicas, so it’s important that the batch size be divisible by the number\n",
        "of replicas. And that’s all! Training will generally be significantly faster than using a\n",
        "single device, and the code change was really minimal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5YrR2Ntiu6b"
      },
      "source": [
        "#### Running on a subset of availablel devices\n",
        "If you only want to use a subset of all the available GPU devices, you can pass the list\n",
        "to the MirroredStrategy’s constructor:\n",
        "\n",
        "`distribution = tf.distribute.MirroredStrategy([\"/gpu:0\", \"/gpu:1\"])`\n",
        "\n",
        "By default, the MirroredStrategy class uses the NVIDIA Collective Communications\n",
        "Library (NCCL) for the AllReduce mean operation, but you can change it by setting\n",
        "the `cross_device_ops` argument to an instance of the `tf.distribute.Hierarchical\n",
        "CopyAllReduce` class, or an instance of the `tf.distribute.ReductionToOneDevice`\n",
        "class. The default NCCL option is based on the `tf.distribute.NcclAllReduce` class,\n",
        "which is usually faster, but this depends on the number and types of GPUs, so you\n",
        "may want to give the alternatives a try."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93PRPd55iu6b"
      },
      "source": [
        "#### Data parallelism with centralized parameters\n",
        "\n",
        "If you want to try using data parallelism with centralized parameters, replace the\n",
        "`MirroredStrategy` with the `CentralStorageStrategy`:\n",
        "\n",
        "`distribution = tf.distribute.experimental.CentralStorageStrategy()`\n",
        "\n",
        "You can optionally set the `compute_devices` argument to specify the list of devices\n",
        "you want to use as workers (by default it will use all available GPUs), and you can\n",
        "optionally set the `parameter_device` argument to specify the device you want to store\n",
        "the parameters on (by default it will use the CPU, or the GPU if there is just one)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-26T09:17:14.928053Z",
          "start_time": "2023-01-26T09:17:14.799666Z"
        },
        "id": "TM_DGnSdiu6c"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "distribution = tf.distribute.MirroredStrategy()\n",
        "\n",
        "# Change the default all-reduce algorithm:\n",
        "distribution = tf.distribute.MirroredStrategy(\n",
        "    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
        "\n",
        "# Option 1: specify the list of GPUs to use:\n",
        "#distribution = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
        "#distribution = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\"])\n",
        "\n",
        "# Option 2: use the central storage strategy instead:\n",
        "#distribution = tf.distribute.experimental.CentralStorageStrategy()\n",
        "\n",
        "# Option 3: if running on Colab and want to use TPU processor\n",
        "#if IS_COLAB and \"COLAB_TPU_ADDR\" in os.environ:\n",
        "#  tpu_address = \"grpc://\" + os.environ[\"COLAB_TPU_ADDR\"]\n",
        "#else:\n",
        "#  tpu_address = \"\"\n",
        "#resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu_address)\n",
        "#tf.config.experimental_connect_to_cluster(resolver)\n",
        "#tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "#distribution = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "\n",
        "with distribution.scope():\n",
        "    model = create_model()\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                  optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
        "                  metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-26T09:40:45.189467Z",
          "start_time": "2023-01-26T09:17:25.145825Z"
        },
        "id": "MIASFTq3iu6c",
        "outputId": "a3b50db1-0547-4178-ebba-f05896f5b586"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-01-26 17:17:25.455915: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "550/550 [==============================] - ETA: 0s - loss: 1.3348 - accuracy: 0.5830"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-01-26 17:19:16.393729: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "550/550 [==============================] - 116s 210ms/step - loss: 1.3348 - accuracy: 0.5830 - val_loss: 0.3170 - val_accuracy: 0.9134\n",
            "Epoch 2/10\n",
            "550/550 [==============================] - ETA: 0s - loss: 0.4206 - accuracy: 0.8719"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-01-26 17:21:16.672896: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "550/550 [==============================] - 120s 217ms/step - loss: 0.4206 - accuracy: 0.8719 - val_loss: 0.1748 - val_accuracy: 0.9502\n",
            "Epoch 3/10\n",
            "550/550 [==============================] - ETA: 0s - loss: 0.2838 - accuracy: 0.9168"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-01-26 17:24:09.213952: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "550/550 [==============================] - 175s 318ms/step - loss: 0.2838 - accuracy: 0.9168 - val_loss: 0.1213 - val_accuracy: 0.9658\n",
            "Epoch 4/10\n",
            "550/550 [==============================] - ETA: 0s - loss: 0.2253 - accuracy: 0.9341"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-01-26 17:27:17.035505: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "550/550 [==============================] - 188s 342ms/step - loss: 0.2253 - accuracy: 0.9341 - val_loss: 0.1001 - val_accuracy: 0.9720\n",
            "Epoch 5/10\n",
            "550/550 [==============================] - ETA: 0s - loss: 0.1929 - accuracy: 0.9451"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-01-26 17:30:24.865641: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "550/550 [==============================] - 187s 341ms/step - loss: 0.1929 - accuracy: 0.9451 - val_loss: 0.0844 - val_accuracy: 0.9760\n",
            "Epoch 6/10\n",
            "550/550 [==============================] - ETA: 0s - loss: 0.1690 - accuracy: 0.9515"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-01-26 17:32:46.056584: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "550/550 [==============================] - 139s 252ms/step - loss: 0.1690 - accuracy: 0.9515 - val_loss: 0.0766 - val_accuracy: 0.9782\n",
            "Epoch 7/10\n",
            "550/550 [==============================] - ETA: 0s - loss: 0.1496 - accuracy: 0.9564"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-01-26 17:34:41.832217: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "550/550 [==============================] - 116s 210ms/step - loss: 0.1496 - accuracy: 0.9564 - val_loss: 0.0715 - val_accuracy: 0.9806\n",
            "Epoch 8/10\n",
            "550/550 [==============================] - ETA: 0s - loss: 0.1402 - accuracy: 0.9595"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-01-26 17:36:41.331472: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "550/550 [==============================] - 119s 217ms/step - loss: 0.1402 - accuracy: 0.9595 - val_loss: 0.0660 - val_accuracy: 0.9818\n",
            "Epoch 9/10\n",
            "550/550 [==============================] - ETA: 0s - loss: 0.1307 - accuracy: 0.9620"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-01-26 17:38:41.368211: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "550/550 [==============================] - 120s 218ms/step - loss: 0.1307 - accuracy: 0.9620 - val_loss: 0.0612 - val_accuracy: 0.9818\n",
            "Epoch 10/10\n",
            "550/550 [==============================] - ETA: 0s - loss: 0.1213 - accuracy: 0.9652"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-01-26 17:40:41.439247: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
            "550/550 [==============================] - 120s 218ms/step - loss: 0.1213 - accuracy: 0.9652 - val_loss: 0.0601 - val_accuracy: 0.9828\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f29916ba640>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_size = 100 # must be divisible by the number of workers\n",
        "model.fit(X_train, y_train, epochs=10,\n",
        "          validation_data=(X_valid, y_valid), batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXe9_Ohhiu6d"
      },
      "source": [
        "If you have multiple GPUs you run the experiment on one and then 2 or more GPUs. With more GPUs, you would see a speedup compared to the training on one GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwR5KN40iu6d"
      },
      "source": [
        "Once you have finished training your model, you can use it to make predictions efficiently:\n",
        "call the `predict()` method, and it will automatically split the batch across all\n",
        "replicas, making predictions in parallel (again, the batch size must be divisible by the\n",
        "number of replicas). \n",
        "\n",
        "If you call the model’s `save()` method, it will be saved as a regular\n",
        "model, not as a mirrored model with multiple replicas. So when you load it, it will\n",
        "run like a regular model, on a single device (by default GPU 0, or the CPU if there are\n",
        "no GPUs). \n",
        "If you want to load a model and run it on all available devices, you must\n",
        "call `keras.models.load_model()` within a distribution context, like:\n",
        "\n",
        "`with distribution.scope():\n",
        "    mirrored_model = keras.models.load_model(\"my_mnist_model.h5\")`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqQPgkUMiu6d"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-26T09:41:44.186359Z",
          "start_time": "2023-01-26T09:41:42.945936Z"
        },
        "id": "rrsFz964iu6d",
        "outputId": "1199879c-9e1b-4f6c-832c-a1f8ec1a151f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.10385561, 0.10114489, 0.09581841, 0.0978876 , 0.09493201,\n",
              "        0.1048363 , 0.09678087, 0.10308881, 0.09845111, 0.10320438],\n",
              "       [0.10551135, 0.10397926, 0.0933008 , 0.09837992, 0.09383032,\n",
              "        0.1024676 , 0.09671868, 0.10909896, 0.09808642, 0.09862671],\n",
              "       [0.10361055, 0.1010047 , 0.09801278, 0.09792618, 0.09564402,\n",
              "        0.10135707, 0.09678063, 0.10341027, 0.09996779, 0.10228598]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "model.predict(X_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQnnGFm_iu6e"
      },
      "source": [
        "Custom training loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-26T10:02:33.741987Z",
          "start_time": "2023-01-26T09:42:05.335129Z"
        },
        "scrolled": false,
        "id": "W9VTFOuIiu6e",
        "outputId": "3f9715f6-609b-4a7a-aaa9-b3908a79ab11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-14-acb7c62c8738>:36: DistributedIteratorV1.initialize (from tensorflow.python.distribute.v1.input_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use the iterator's `initializer` property instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:459: StrategyBase.experimental_run (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "use run() instead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Loss: 0.409\n",
            "Epoch 2/10\n",
            "Loss: 0.319\n",
            "Epoch 3/10\n",
            "Loss: 0.293\n",
            "Epoch 4/10\n",
            "Loss: 0.283\n",
            "Epoch 5/10\n",
            "Loss: 0.278\n",
            "Epoch 6/10\n",
            "Loss: 0.276\n",
            "Epoch 7/10\n",
            "Loss: 0.274\n",
            "Epoch 8/10\n",
            "Loss: 0.265\n",
            "Epoch 9/10\n",
            "Loss: 0.255\n",
            "Epoch 10/10\n",
            "Loss: 0.249\n"
          ]
        }
      ],
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "K = keras.backend\n",
        "\n",
        "distribution = tf.distribute.MirroredStrategy()\n",
        "\n",
        "with distribution.scope():\n",
        "    model = create_model()\n",
        "    optimizer = keras.optimizers.SGD()\n",
        "\n",
        "with distribution.scope():\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).repeat().batch(batch_size)\n",
        "    input_iterator = distribution.make_dataset_iterator(dataset)\n",
        "    \n",
        "@tf.function\n",
        "def train_step():\n",
        "    def step_fn(inputs):\n",
        "        X, y = inputs\n",
        "        with tf.GradientTape() as tape:\n",
        "            Y_proba = model(X)\n",
        "            loss = K.sum(keras.losses.sparse_categorical_crossentropy(y, Y_proba)) / batch_size\n",
        "\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    per_replica_losses = distribution.experimental_run(step_fn, input_iterator)\n",
        "    mean_loss = distribution.reduce(tf.distribute.ReduceOp.SUM,\n",
        "                                    per_replica_losses, axis=None)\n",
        "    return mean_loss\n",
        "\n",
        "n_epochs = 10\n",
        "with distribution.scope():\n",
        "    input_iterator.initialize()\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n",
        "        for iteration in range(len(X_train) // batch_size):\n",
        "            print(\"\\rLoss: {:.3f}\".format(train_step().numpy()), end=\"\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg0xkneViu6e"
      },
      "source": [
        "## Training across multiple servers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wnGzwfliu6f"
      },
      "source": [
        "A TensorFlow cluster is a group of TensorFlow processes running in parallel, usually on different machines, and talking to each other to complete some work, for example training or executing a neural network. Each TF process in the cluster is called a \"task\" (or a \"TF server\"). It has an IP address, a port, and a type (also called its role or its job). The type can be `\"worker\"`, `\"chief\"`, `\"ps\"` (parameter server) or `\"evaluator\"`:\n",
        "* Each **worker** performs computations, usually on a machine with one or more GPUs.\n",
        "* The **chief** performs computations as well, but it also handles extra work such as writing TensorBoard logs or saving checkpoints. There is a single chief in a cluster, typically the first worker (i.e., worker #0).\n",
        "* A **parameter server** (ps) only keeps track of variable values, it is usually on a CPU-only machine.\n",
        "* The **evaluator** obviously takes care of evaluation. There is usually a single evaluator in a cluster.\n",
        "\n",
        "The set of tasks that share the same type is often called a \"job\". For example, the \"worker\" job is the set of all workers.\n",
        "\n",
        "To start a TensorFlow cluster, you must first define it. This means specifying all the tasks (IP address, TCP port, and type). For example, the following cluster specification defines a cluster with 3 tasks (2 workers and 1 parameter server). It's a dictionary with one key per job, and the values are lists of task addresses:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DwToUsbwiu6f"
      },
      "outputs": [],
      "source": [
        "cluster_spec = {\n",
        "    \"worker\": [\n",
        "        \"machine-a.example.com:2222\",  # /job:worker/task:0\n",
        "        \"machine-b.example.com:2222\"   # /job:worker/task:1\n",
        "    ],\n",
        "    \"ps\": [\"machine-c.example.com:2222\"] # /job:ps/task:0\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phs_KmAmiu6f"
      },
      "source": [
        "Every task in the cluster may communicate with every other task in the server, so make sure to configure your firewall to authorize all communications between these machines on these ports (it's usually simpler if you use the same port on every machine).\n",
        "\n",
        "When a task is started, it needs to be told which one it is: its type and index (the task index is also called the task id). A common way to specify everything at once (both the cluster spec and the current task's type and id) is to set the `TF_CONFIG` environment variable before starting the program. It must be a JSON-encoded dictionary containing a cluster specification (under the `\"cluster\"` key), and the type and index of the task to start (under the `\"task\"` key). For example, the following `TF_CONFIG` environment variable defines the same cluster as above, with 2 workers and 1 parameter server, and specifies that the task to start is worker #1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "620P4Gn4iu6g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "b7df6c39-f3f9-4cb9-c61f-b16511d63f21"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"cluster\": {\"worker\": [\"machine-a.example.com:2222\", \"machine-b.example.com:2222\"], \"ps\": [\"machine-c.example.com:2222\"]}, \"task\": {\"type\": \"worker\", \"index\": 1}}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "os.environ[\"TF_CONFIG\"] = json.dumps({\n",
        "    \"cluster\": cluster_spec,\n",
        "    \"task\": {\"type\": \"worker\", \"index\": 1}\n",
        "})\n",
        "os.environ[\"TF_CONFIG\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RePB_dEdiu6h"
      },
      "source": [
        "Some platforms (e.g., Google Cloud ML Engine) automatically set this environment variable for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1CgaHvniu6h"
      },
      "source": [
        "TensorFlow's `TFConfigClusterResolver` class reads the cluster configuration from this environment variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "NsjABDfsiu6h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e131161-1afe-4f58-c5ea-a913ff878fe1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ClusterSpec({'ps': ['machine-c.example.com:2222'], 'worker': ['machine-a.example.com:2222', 'machine-b.example.com:2222']})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\n",
        "resolver.cluster_spec()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "y2bEGI7ziu6h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "4e5b3b81-4291-431c-ea86-153541c7c4c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'worker'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "resolver.task_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "KCK22-QXiu6h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4e60fab-7e93-4d29-9118-1c978d43ad38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "resolver.task_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6GA6EuWiu6i"
      },
      "source": [
        "Now let's run a simpler cluster with just two worker tasks, both running on the local machine. We will use the `MultiWorkerMirroredStrategy` to train a model across these two tasks.\n",
        "\n",
        "The first step is to write the training code. As this code will be used to run both workers, each in its own process, we write this code to a separate Python file, `my_mnist_multiworker_task.py`. The code is relatively straightforward, but there are a couple important things to note:\n",
        "* We create the `MultiWorkerMirroredStrategy` before doing anything else with TensorFlow.\n",
        "* Only one of the workers will take care of logging to TensorBoard and saving checkpoints. As mentioned earlier, this worker is called the *chief*, and by convention it is usually worker #0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-Sy1hk2Qiu6i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f464b6b-f7f2-4045-d975-f44b2854397e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing my_mnist_multiworker_task.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile my_mnist_multiworker_task.py\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import time\n",
        "\n",
        "# At the beginning of the program\n",
        "distribution = tf.distribute.MultiWorkerMirroredStrategy()\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\n",
        "print(\"Starting task {}{}\".format(resolver.task_type, resolver.task_id))\n",
        "\n",
        "# Only worker #0 will write checkpoints and log to TensorBoard\n",
        "if resolver.task_id == 0:\n",
        "    root_logdir = os.path.join(os.curdir, \"my_mnist_multiworker_logs\")\n",
        "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
        "    run_dir = os.path.join(root_logdir, run_id)\n",
        "    callbacks = [\n",
        "        keras.callbacks.TensorBoard(run_dir),\n",
        "        keras.callbacks.ModelCheckpoint(\"my_mnist_multiworker_model.h5\",\n",
        "                                        save_best_only=True),\n",
        "    ]\n",
        "else:\n",
        "    callbacks = []\n",
        "\n",
        "# Load and prepare the MNIST dataset\n",
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "X_train_full = X_train_full[..., np.newaxis] / 255.\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "\n",
        "with distribution.scope():\n",
        "    model = keras.models.Sequential([\n",
        "        keras.layers.Conv2D(filters=64, kernel_size=7, activation=\"relu\",\n",
        "                            padding=\"same\", input_shape=[28, 28, 1]),\n",
        "        keras.layers.MaxPooling2D(pool_size=2),\n",
        "        keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\",\n",
        "                            padding=\"same\"), \n",
        "        keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\",\n",
        "                            padding=\"same\"),\n",
        "        keras.layers.MaxPooling2D(pool_size=2),\n",
        "        keras.layers.Flatten(),\n",
        "        keras.layers.Dense(units=64, activation='relu'),\n",
        "        keras.layers.Dropout(0.5),\n",
        "        keras.layers.Dense(units=10, activation='softmax'),\n",
        "    ])\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                  optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
        "                  metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
        "          epochs=10, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKw7tC2Ciu6i"
      },
      "source": [
        "In a real world application, there would typically be a single worker per machine, but in this example we're running both workers on the same machine, so they will both try to use all the available GPU RAM (if this machine has a GPU), and this will likely lead to an Out-Of-Memory (OOM) error. To avoid this, we could use the `CUDA_VISIBLE_DEVICES` environment variable to assign a different GPU to each worker. Alternatively, we can simply disable GPU support, like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6pOsrg3ziu6i"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz_GlV4Xiu6j"
      },
      "source": [
        "We are now ready to start both workers, each in its own process, using Python's `subprocess` module. Before we start each process, we need to set the `TF_CONFIG` environment variable appropriately, changing only the task index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-OGsI-oIiu6j"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "cluster_spec = {\"worker\": [\"127.0.0.1:9901\", \"127.0.0.1:9902\"]}\n",
        "\n",
        "for index, worker_address in enumerate(cluster_spec[\"worker\"]):\n",
        "    os.environ[\"TF_CONFIG\"] = json.dumps({\n",
        "        \"cluster\": cluster_spec,\n",
        "        \"task\": {\"type\": \"worker\", \"index\": index}\n",
        "    })\n",
        "    subprocess.Popen(\"python my_mnist_multiworker_task.py\", shell=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49nqGWebiu6j"
      },
      "source": [
        "That's it! Our TensorFlow cluster is now running, but we can't see it in this notebook because it's running in separate processes (but if you are running this notebook in Jupyter, you can see the worker logs in Jupyter's server logs).\n",
        "\n",
        "Since the chief (worker #0) is writing to TensorBoard, we use TensorBoard to view the training progress. Run the following cell, then click on the settings button (i.e., the gear icon) in the TensorBoard interface and check the \"Reload data\" box to make TensorBoard automatically refresh every 30s. Once the first epoch of training is finished (which may take a few minutes), and once TensorBoard refreshes, the SCALARS tab will appear. Click on this tab to view the progress of the model's training and validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JC33g0itiu6k"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=./my_mnist_multiworker_logs --port=6006"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ygPt59Wiu6k"
      },
      "source": [
        "That's it! Once training is over, the best checkpoint of the model will be available in the `my_mnist_multiworker_model.h5` file. You can load it using `keras.models.load_model()` and use it for predictions, as usual:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "aXdB_9XQiu6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7bb5bbd-df3a-4d9d-8b15-e23619714bc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157/157 [==============================] - 1s 3ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, ..., 2, 1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "model = keras.models.load_model(\"my_mnist_multiworker_model.h5\")\n",
        "Y_pred = model.predict(X_valid)\n",
        "np.argmax(Y_pred, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYZKFq0Kiu6l"
      },
      "source": [
        "And that's all for today! Hope you found this useful. 😊"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_lsTm4Biu6l"
      },
      "source": [
        "# Exercise Solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37bVg-Qqiu6l"
      },
      "source": [
        "## 1. to 8.\n",
        "\n",
        "See Appendix A."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fatrQyuqiu6l"
      },
      "source": [
        "## 9.\n",
        "_Exercise: Train a model (any model you like) and deploy it to TF Serving or Google Cloud AI Platform. Write the client code to query it using the REST API or the gRPC API. Update the model and deploy the new version. Your client code will now query the new version. Roll back to the first version._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai31PBM3iu6m"
      },
      "source": [
        "Please follow the steps in the <a href=\"#Deploying-TensorFlow-models-to-TensorFlow-Serving-(TFS)\">Deploying TensorFlow models to TensorFlow Serving</a> section above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qqzh0Rmiu6m"
      },
      "source": [
        "# 10.\n",
        "_Exercise: Train any model across multiple GPUs on the same machine using the `MirroredStrategy` (if you do not have access to GPUs, you can use Colaboratory with a GPU Runtime and create two virtual GPUs). Train the model again using the `CentralStorageStrategy `and compare the training time._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVRfFM9-iu6m"
      },
      "source": [
        "Please follow the steps in the [Distributed Training](#Distributed-Training) section above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bV1hRaviu6m"
      },
      "source": [
        "# 11.\n",
        "_Exercise: Train a small model on Google Cloud AI Platform, using black box hyperparameter tuning._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3cjwFsuiu6n"
      },
      "source": [
        "Please follow the instructions on pages 716-717 of the book. You can also read [this documentation page](https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview) and go through the example in this nice [blog post](https://towardsdatascience.com/how-to-do-bayesian-hyper-parameter-tuning-on-a-blackbox-model-882009552c6d) by Lak Lakshmanan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1Zd_tk4iu6n"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}